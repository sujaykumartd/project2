{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb775c99-7432-4c9a-af42-9fff035d8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn \n",
    "# !pip3 install numpy==2.0\n",
    "# !pip install kneed \n",
    "# !pip install umap-learn\n",
    "# !pip install multiprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c770f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 12:15:09.359231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering,HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "# from tqdm import tqdm \n",
    "# import multiprocessing as mp\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be6e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de65064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimension reduction class\n",
    "# class dimension_redution_optimiser:\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "#         self.pca_threshold = 0.95\n",
    "#         self.tsne_min_components = 2\n",
    "#         self.tsne_max_components = 3\n",
    "#         self.umap_min_components = 2\n",
    "#         self.umap_max_components = 6\n",
    "          \n",
    "#     def pca_n_components_optimizer(self):\n",
    "#         print('  ** Selecting best N component for PCA')\n",
    "#         # Initialize PCA\n",
    "#         pca = PCA()\n",
    "\n",
    "#         # Fit PCA\n",
    "#         X_pca = pca.fit_transform(self.data)\n",
    "#         cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "#         self.best_n_components = len(cumulative_variance[cumulative_variance <=  self.pca_threshold]) #select PC covers 95variance\n",
    "#         print(f'  ** best N component: {self.best_n_components}')\n",
    "         \n",
    "    \n",
    "\n",
    "#     def tsne_n_components_optimizer(self):\n",
    "#         print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "#         best_n_components = None\n",
    "#         best_score = -1\n",
    "\n",
    "#         for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "#             tsne = TSNE(n_components=n_components)\n",
    "#             transformed_data = tsne.fit_transform(self.data)\n",
    "# #             print(n_components)\n",
    "            \n",
    "#             # Example with KMeans clustering (can replace with other methods)\n",
    "#             kmeans = KMeans(n_clusters=4)\n",
    "#             kmeans.fit(transformed_data)\n",
    "#             labels = kmeans.labels_\n",
    "            \n",
    "#             if(len(set(labels))>1):\n",
    "#                 score = silhouette_score(transformed_data, labels)\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_n_components = n_components\n",
    "#         self.best_n_components = best_n_components\n",
    "#         print(f'  ** best N component: {self.best_n_components}')\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "#     def umap_n_components_optimizer(self):\n",
    "#         print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "        \n",
    "#         best_n_components = None\n",
    "#         best_score = -1\n",
    "\n",
    "#         for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "#             print(n_components)\n",
    "#             reducer = umap.UMAP(n_components=n_components)\n",
    "#             reduced_data = reducer.fit_transform(self.data)\n",
    "            \n",
    "#             # Example with KMeans clustering (can replace with other methods)\n",
    "#             kmeans = KMeans(n_clusters=4)\n",
    "#             kmeans.fit(reduced_data)\n",
    "#             labels = kmeans.labels_\n",
    "#             if(len(set(labels))>1):\n",
    "#                 score = silhouette_score(reduced_data, labels)\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_n_components = n_components\n",
    "#             print(n_components)\n",
    "            \n",
    "\n",
    "#         self.best_n_components = best_n_components\n",
    "#         print(f'  ** best N component: {self.best_n_components}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a7d9ac-f1b3-4eb7-8ed2-c291b96e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DimensionReductionOptimiser:\n",
    "#     def __init__(self, data,scaling_methods,dim_reduction_methods):\n",
    "#         self.data_dict = data\n",
    "#         self.scaling_methods = scaling_methods\n",
    "#         self.dim_reduction_methods = dim_reduction_methods\n",
    "#         self.pca_threshold = 0.95\n",
    "#         self.tsne_min_components = 2\n",
    "#         self.tsne_max_components = 3\n",
    "#         self.umap_min_components = 2\n",
    "#         self.umap_max_components = 6\n",
    "#         self.n_components_dict = {method:[] for method in methods}\n",
    "          \n",
    "#     def pca_n_components_optimizer(self,scaling_method):\n",
    "#         print('  ** Selecting best N component for PCA')\n",
    "#         # Initialize PCA\n",
    "#         pca = PCA()\n",
    "\n",
    "#         # Fit PCA\n",
    "#         X_pca = pca.fit_transform(self.data_dict[scaling_method])\n",
    "#         cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "#         best_n_components = len(cumulative_variance[cumulative_variance <= self.pca_threshold])  # Select PCs covering 95% variance\n",
    "#         print(f'  ** best N component: {best_n_components}')\n",
    "#         return  best_n_components\n",
    "\n",
    "#     def tsne_n_components_optimizer(self,scaling_method):\n",
    "#         print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "#         best_n_components = None\n",
    "#         best_score = -1\n",
    "\n",
    "#         for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "#             print(f'TSNE {n_components}')\n",
    "            \n",
    "#             tsne = TSNE(n_components=n_components)\n",
    "#             transformed_data = tsne.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "#             # Example with KMeans clustering (can replace with other methods)\n",
    "#             kmeans = KMeans(n_clusters=4)\n",
    "#             kmeans.fit(transformed_data)\n",
    "#             labels = kmeans.labels_\n",
    "            \n",
    "#             if len(set(labels)) > 1:\n",
    "#                 score = silhouette_score(transformed_data, labels)\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_n_components = n_components\n",
    "\n",
    "#         print(f'  ** best N component for TSNE: {best_n_components}')\n",
    "#         return  best_n_components\n",
    "\n",
    "#     def umap_n_components_optimizer(self,scaling_method):\n",
    "#         print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "#         best_n_components = None\n",
    "#         best_score = -1\n",
    "\n",
    "#         for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "#             print(f'umap {n_components}')\n",
    "#             reducer = umap.UMAP(n_components=n_components)\n",
    "#             reduced_data = reducer.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "#             # Example with KMeans clustering (can replace with other methods)\n",
    "#             kmeans = KMeans(n_clusters=4)\n",
    "#             kmeans.fit(reduced_data)\n",
    "#             labels = kmeans.labels_\n",
    "            \n",
    "#             if len(set(labels)) > 1:\n",
    "#                 score = silhouette_score(reduced_data, labels)\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_n_components = n_components\n",
    "\n",
    "#         print(f'  ** best N component for UMAP: {best_n_components}')\n",
    "#         return best_n_components\n",
    "    \n",
    "#     def optimize_all(self):\n",
    "#         func_dict = {'pca'\n",
    "            \n",
    "#         }\n",
    "#         with ProcessPoolExecutor() as executor:\n",
    "#             # Submit tasks to the executor\n",
    "#             futures = {\n",
    "#                 executor.submit(self.pca_n_components_optimizer): 'PCA',\n",
    "#                 executor.submit(self.tsne_n_components_optimizer): 'TSNE',\n",
    "#                 executor.submit(self.umap_n_components_optimizer): 'UMAP'\n",
    "#             }\n",
    "            \n",
    "#             for future in as_completed(futures):\n",
    "#                 method = futures[future]\n",
    "#                 try:\n",
    "#                     result = future.result()\n",
    "#                     self.n_components_dict[method] = result\n",
    "#                     print(f'{method} result: {result}')\n",
    "#                 except Exception as exc:\n",
    "#                     print(f'{method} generated an exception: {exc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afc6568c-2f3c-454d-bd5b-fd70e0b309a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionReductionOptimiser:\n",
    "    def __init__(self, data,scaling_methods,dim_reduction_methods):\n",
    "        self.data_dict = data\n",
    "        self.scaling_methods = scaling_methods\n",
    "        self.dim_reduction_methods = dim_reduction_methods\n",
    "        self.pca_threshold = 0.95\n",
    "        self.tsne_min_components = 2\n",
    "        self.tsne_max_components = 3\n",
    "        self.umap_min_components = 2\n",
    "        self.umap_max_components = 6\n",
    "        self.n_components_dict = {scale:{dim:None} for dim in dim_reduction_methods for scale in scaling_methods}\n",
    "          \n",
    "    def pca_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for PCA')\n",
    "        # Initialize PCA\n",
    "        pca = PCA()\n",
    "\n",
    "        # Fit PCA\n",
    "        X_pca = pca.fit_transform(self.data_dict[scaling_method])\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        best_n_components = len(cumulative_variance[cumulative_variance <= self.pca_threshold])  # Select PCs covering 95% variance\n",
    "        print(f'  ** best N component: {best_n_components}')\n",
    "        return  best_n_components\n",
    "\n",
    "    def tsne_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "            print(f'TSNE {n_components}')\n",
    "            \n",
    "            tsne = TSNE(n_components=n_components)\n",
    "            transformed_data = tsne.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(transformed_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(transformed_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for TSNE: {best_n_components}')\n",
    "        return  best_n_components\n",
    "\n",
    "    def umap_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "            print(f'umap {n_components}')\n",
    "            reducer = umap.UMAP(n_components=n_components)\n",
    "            reduced_data = reducer.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(reduced_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(reduced_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for UMAP: {best_n_components}')\n",
    "        return best_n_components\n",
    "    \n",
    "    def optimize_all(self):\n",
    "        func_list = [(scale,dim) for dim in self.dim_reduction_methods for scale in self.scaling_methods]\n",
    "        \n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {executor.submit(getattr(self,f'{dim.lower()}_n_components_optimizer'),scaling): (scaling,dim) for scaling,dim in func_list}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    self.n_components_dict[method[0]][method[1]] = result\n",
    "                    print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10181591-7de0-47d7-a835-57bc0c202dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "# Class to fetch the best n_cluster parameter for the clustering models\n",
    "\n",
    "class clustering_model_optimizer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.min_clusters = 4\n",
    "        self.max_clusters = 10\n",
    "        \n",
    "        \n",
    "    def kmeans_n_cluster_optimizer(self):\n",
    "        print('  *** Entered the elbow_plot method of the KMeansClustering class')\n",
    "        wcss=[] # initializing an empty list\n",
    "        for i in range (self.min_clusters,self.max_clusters+1):\n",
    "            print(i)\n",
    "            kmeans=KMeans(n_clusters=i,random_state=42) # initializing the KMeans object\n",
    "            print(i)\n",
    "            \n",
    "            kmeans.fit(self.data) # fitting the data to the KMeans Algorithm\n",
    "            print(i)\n",
    "            \n",
    "            wcss.append(kmeans.inertia_)\n",
    "            \n",
    "        kn = KneeLocator(range(self.min_clusters,self.max_clusters+1), wcss, curve='convex', direction='decreasing')\n",
    "        print('completed')\n",
    "        self.best_N_cluster =  kn.knee    \n",
    "\n",
    "    def agglomerative_spectural_n_cluster_optimizer(self,model_name):\n",
    "        # print(f\"selecting best n_cluster parameter for model:{model_name}\")\n",
    "        score_dict = {'n_clusters':[],\n",
    "                     'silhouette_score':[],\n",
    "                     'DB_score':[]}\n",
    "\n",
    "        for n_clusters in range( self.min_clusters, self.max_clusters + 1):\n",
    "            if model_name == 'agglomerative':\n",
    "                model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            elif model_name == 'spectral':\n",
    "                # print(n_clusters)\n",
    "                model = SpectralClustering(n_clusters=n_clusters)\n",
    "            else:\n",
    "                raise ValueError('Unknown cluster model name')\n",
    "                \n",
    "            labels = model.fit_predict(self.data)\n",
    "            silhouette_score_ = silhouette_score(self.data, labels)\n",
    "            davies_bouldin_score_ = davies_bouldin_score(self.data, labels)\n",
    "            score_dict['n_clusters'].append(n_clusters)\n",
    "            score_dict['silhouette_score'].append(silhouette_score_)\n",
    "            score_dict['DB_score'].append(davies_bouldin_score_)\n",
    "        score_df = pd.DataFrame(score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])        \n",
    "        self.best_N_cluster = score_df.n_clusters[0]\n",
    "        \n",
    "    def agglomerative_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='agglomerative')\n",
    "    def spectral_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f97e6204-67d9-46e0-aba8-5d44d37a5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class best_clustering_model_selector:\n",
    "    def __init__(self, df):\n",
    "        print('-'*50)\n",
    "        print('Data Loaded')\n",
    "        self.labels = []\n",
    "        self.best_n_cluster = None\n",
    "        self.data = df\n",
    "        self.scalers = {\n",
    "                        'normalization': MinMaxScaler(),\n",
    "                        'standardization': StandardScaler()\n",
    "                        }\n",
    "        self.dimension_reduction={\n",
    "                        'PCA': PCA(),\n",
    "                        'UMAP':umap.UMAP(n_jobs=-1),\n",
    "                        'TSNE': TSNE()\n",
    "                        }\n",
    "\n",
    "        self.clusters = {\n",
    "                        'KMeans':KMeans(random_state=42),\n",
    "                        'DBSCAN':DBSCAN(eps=0.5, min_samples=5),\n",
    "                        \n",
    "                        'Agglomerative':AgglomerativeClustering(),\n",
    "                        'Spectral':SpectralClustering(random_state=42)\n",
    "                        }\n",
    "        self.feature_engineering()\n",
    "        \n",
    "        \n",
    "    def prepare_data_for_preprocessing(self):\n",
    "    # Build preprocessing pipeline for categorical features\n",
    "        print(self.data.shape)\n",
    "        self.data.set_index('Customer ID',inplace=True)\n",
    "        print(self.data.shape)\n",
    "        one_hot_cat_col = []\n",
    "        ordinal_cat_col = []\n",
    "        # creating dummies\n",
    "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = self.data.select_dtypes(include=['object']).columns\n",
    "        for feature in categorical_features:\n",
    "            unique_values = self.data[feature].nunique()\n",
    "            if unique_values < 4:\n",
    "                one_hot_cat_col.append(feature)\n",
    "            else:\n",
    "                ordinal_cat_col.append(feature)\n",
    "        self.one_hot_cat_col = one_hot_cat_col\n",
    "        self.ordinal_cat_col = ordinal_cat_col\n",
    "        self.numerical_col = numerical_features\n",
    "        \n",
    "    def data_preprocess_pipeline(self):\n",
    "        # Build preprocessing pipeline for numerical features\n",
    "        print(\"** KNN imputer for missing values\")\n",
    "        numerical_pipeline = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        #     ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "        # Build preprocessing pipeline for categorical features\n",
    "        print(\"** One hot encoding for the Categorical features with less than 4 unique values\")\n",
    "        \n",
    "        onehot_categorical_pipeline = Pipeline(steps=[\n",
    "            ('onehot_encoding', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "        \n",
    "        print(\"** Ordinal encoding for the Categorical features with greater than 4 unique values\")\n",
    "\n",
    "        ordinal_categorical_pipeline = Pipeline(steps=[\n",
    "            ('ordinal_encoding', OrdinalEncoder())\n",
    "\n",
    "        ])\n",
    "        column_transform = ColumnTransformer([\n",
    "            ('numerical_columns', numerical_pipeline, self.numerical_col),\n",
    "            ('onehot_categorical_columns', onehot_categorical_pipeline, self.one_hot_cat_col),\n",
    "            ('ordinal_categorical_columns', ordinal_categorical_pipeline, self.ordinal_cat_col)   \n",
    "        ])\n",
    "        self.trans_df = pd.DataFrame(column_transform.fit_transform(self.data)) \n",
    "        one_hot_cols = list(onehot_categorical_pipeline.fit(self.data[self.one_hot_cat_col]).get_feature_names_out())   \n",
    "        trans_cols = list(self.numerical_col)+one_hot_cols+list(self.ordinal_cat_col)  \n",
    "        self.trans_df.columns = trans_cols \n",
    "        \n",
    "        \n",
    "    def remove_constant_multicolinear_feature(self):\n",
    "        ### Removing constant features\n",
    "        print('** Removing Constant features with 0 std')\n",
    "        std_df = self.trans_df.describe().T['std']\n",
    "        const_feature_list = list(std_df[std_df==0].index)\n",
    "        if len(const_feature_list) > 0:\n",
    "            self.trans_df.drop(const_feature_list,axis=1,inplace=True)\n",
    "\n",
    "        ### Reducing number of features using correlation matrix \n",
    "        print('** Removing Multicolinear features')\n",
    "        corr_mat = self.trans_df.corr().abs()\n",
    "        upper_cor_mat_df = corr_mat.where(np.triu(np.ones(corr_mat.shape),k=1).astype(bool))\n",
    "        drop_col = [col for col in upper_cor_mat_df.columns if any(upper_cor_mat_df[col] > 0.85)]\n",
    "\n",
    "        self.data.reset_index(inplace=True)\n",
    "        self.trans_df = pd.concat([self.data['Customer ID'],self.trans_df],axis=1)\n",
    "        self.trans_df.drop(drop_col,axis=1,inplace=True)\n",
    "#         self.trans_df.set_index('Customer ID',inplace=True)\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        print('-'*50)\n",
    "        print('Entering Data preprocessing ')\n",
    "        self.prepare_data_for_preprocessing()\n",
    "        print('--Column transformation')\n",
    "        self.data_preprocess_pipeline()\n",
    "        print('--Feature Removing')\n",
    "        self.remove_constant_multicolinear_feature()\n",
    "        \n",
    "    def scaling_data(self,scaling_method):\n",
    "        if scaling_method in self.scalers.keys():\n",
    "            # Build preprocessing pipeline for numerical features\n",
    "            print(\"** Scaling Numerical features\")\n",
    "            scaling_pipeline = Pipeline(steps=[('scaler', self.scalers[scaling_method])])\n",
    "            temp_df = self.trans_df.drop(self.numerical_col,axis=1)\n",
    "            column_transform = ColumnTransformer([('numerical_columns', scaling_pipeline, self.numerical_col)])\n",
    "            self.scaled_df = pd.DataFrame(column_transform.fit_transform(self.trans_df))\n",
    "            self.scaled_df.columns=self.numerical_col\n",
    "            self.scaled_df = pd.concat([temp_df,self.scaled_df],axis=1)\n",
    "            self.scaled_df.set_index('Customer ID',inplace=True)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown Data Scaling method\")\n",
    "            \n",
    "       \n",
    "    \n",
    "    def reduce_dimensions(self, dimension_method='PCA',scaling_method='normalization'):\n",
    "        if dimension_method in self.dimension_reduction.keys():\n",
    "\n",
    "            # Create an instance of the class\n",
    "            # n_comp_selector = dimension_redution_optimiser(self.scaled_df)\n",
    "\n",
    "            # # Construct the method name dynamically and call it\n",
    "            # method_name = f'{dimension_method.lower()}_n_components_optimizer'\n",
    "            # # Use getattr to call the method\n",
    "            # method = getattr(n_comp_selector, method_name, None)\n",
    "            # if method:\n",
    "            #     method()  # Call the dynamically selected method\n",
    "            #     self.dim_red_n_component = n_comp_selector.best_n_components\n",
    "            #     self.reducer = self.dimension_reduction[dimension_method]\n",
    "            #     self.reducer.n_components = self.dim_red_n_component\n",
    "            # else:\n",
    "            #     raise ValueError(f\"{method} not found\")\n",
    "                        \n",
    "            if dimension_method in self.dimension_reduction.keys():\n",
    "                self.dim_red_n_component = self.reduce_dim_n_component_dict[scaling_method][dimension_method]\n",
    "                self.reducer = self.dimension_reduction[dimension_method]\n",
    "                self.reducer.n_components = self.dim_red_n_component\n",
    "            else:\n",
    "                raise ValueError(f\"{method} not found\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown dimensionality reduction method\")\n",
    "        \n",
    "        self.reduced_df = self.reducer.fit_transform(self.scaling_data_dict[scaling_method])\n",
    "    \n",
    "    \n",
    "    def reduce_data_parallel(self,scaling_methods,dim_reduction_methods):\n",
    "        scaling_data_dict = {scaling:[] for scaling in scaling_methods}\n",
    "        \n",
    "        for scaling in scaling_methods:\n",
    "            self.scaling_data(scaling)\n",
    "            scaling_data_dict[scaling] = self.scaled_df\n",
    "            # Initialize the optimizer class with data\n",
    "        self.scaling_data_dict = scaling_data_dict\n",
    "        optimiser = DimensionReductionOptimiser(self.scaling_data_dict,scaling_methods,dim_reduction_methods)\n",
    "        # Run optimizers in parallel\n",
    "        optimiser.optimize_all()\n",
    "        # scaling_dict[scaling] = optimiser.n_components_dict\n",
    "        self.reduce_dim_n_component_dict = optimiser.n_components_dict\n",
    "    \n",
    "    def clustering_data(self, cluster_method='KMeans'):\n",
    "        if cluster_method in self.clusters.keys():\n",
    "            \n",
    "            if cluster_method != 'DBSCAN':\n",
    "                # Create an instance of the class\n",
    "                n_cluster_selector = clustering_model_optimizer(self.reduced_df)\n",
    "\n",
    "                # Construct the method name dynamically and call it\n",
    "                method_name = f'{cluster_method.lower()}_n_cluster_optimizer'\n",
    "                # Use getattr to call the method\n",
    "                method = getattr(n_cluster_selector, method_name, None)\n",
    "                if method:\n",
    "                    method()  # Call the dynamically selected method\n",
    "                    self.best_n_cluster = n_cluster_selector.best_N_cluster\n",
    "                    self.clusterer = self.clusters[cluster_method]\n",
    "                    if self.best_n_cluster != None: \n",
    "                        self.clusterer.n_clusters = self.best_n_cluster\n",
    "                        self.labels = self.clusterer.fit_predict(self.reduced_df)\n",
    "                    else:\n",
    "                        print(f'{cluster_method} model is not built')\n",
    "                    \n",
    "                else:\n",
    "                    self.labels = []\n",
    "                    \n",
    "                    raise ValueError(f\"{method} not found\")\n",
    "            elif cluster_method == 'DBSCAN':\n",
    "                self.clusterer = self.clusters[cluster_method]\n",
    "                self.labels = self.clusterer.fit_predict(self.reduced_df)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            self.labels = []\n",
    "            \n",
    "            raise ValueError(\"Unknown clustering method\")\n",
    "\n",
    "        # if self.best_n_cluster != None or cluster_method == 'DBSCAN':\n",
    "        #     self.labels = self.clusterer.fit_predict(self.reduced_df)\n",
    "        #     print(f'fit predict for methof {cluster_method}')\n",
    "            \n",
    "        # else:\n",
    "        #     self.labels = []\n",
    "        # print(f'completed cluster_data function dor cluster method {cluster_method}')\n",
    "        return self.labels\n",
    "    \n",
    "    def evaluate_clustering(self):\n",
    "        silhouette = silhouette_score(self.scaled_df, self.labels)\n",
    "        davies_bouldin = davies_bouldin_score(self.scaled_df, self.labels)\n",
    "        return silhouette, davies_bouldin\n",
    "    \n",
    "    def prepare_reduced_data(self,scaling_methods,dim_reduction_methods):\n",
    "        reduce_data_dict ={}\n",
    "        reduced_data_dict={}\n",
    "        scaled_data_dict = {}\n",
    "        for scaling_method in scaling_methods:\n",
    "            print('-'*50)\n",
    "            print(scaling_method)\n",
    "            self.scaling_data(scaling_method)\n",
    "            scaled_data_dict[scaling_method]=self.scaled_df\n",
    "            result_dict = {}\n",
    "            for dim_red_method in dim_reduction_methods:\n",
    "                print('*' * 50)\n",
    "                print(f' --{dim_red_method}')\n",
    "                self.reduce_dimensions(dimension_method=dim_red_method,scaling_method=scaling_method)\n",
    "                result_dict[dim_red_method]={'data':self.reduced_df,'n_components':self.reducer.n_components}\n",
    "#                 print(scaling,dim)\n",
    "            reduced_data_dict[scaling_method] = result_dict\n",
    "        # print(\"Completed...\")\n",
    "        self.reduced_data_dict = reduced_data_dict\n",
    "        self.scaled_data_dict = scaled_data_dict\n",
    "\n",
    "\n",
    "        \n",
    "      #   With paraller processing \n",
    "\n",
    "    # Helper function to perform clustering in parallel\n",
    "    def clustering_worker(self, params):\n",
    "        scaling_method, dim_red_method, cluster_method = params\n",
    "        # self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "        # print('*'*50)\n",
    "        # print(scaling_method, dim_red_method, cluster_method)\n",
    "        # self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        # self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "        self.scaling_data(scaling_method)\n",
    "        self.reduce_dimensions(dimension_method=dim_red_method,scaling_method=scaling_method)\n",
    "        \n",
    "        self.clustering_data(cluster_method=cluster_method)\n",
    "        \n",
    "        if len(set(self.labels)) > 1:\n",
    "            silhouette, davies_bouldin = self.evaluate_clustering()\n",
    "            print('*'*50)\n",
    "            print(scaling_method, dim_red_method, cluster_method)\n",
    "            print('model Evaluation')\n",
    "            return (scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,self.best_n_cluster)\n",
    "        else:\n",
    "            print('*'*50)\n",
    "            print(scaling_method, dim_red_method, cluster_method)\n",
    "            print('model not created')\n",
    "            return None\n",
    "            \n",
    "    def compare_models_parallel(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "        best_score = -float('inf')\n",
    "        best_dim_red_method = None\n",
    "        best_clustering_method = None\n",
    "        best_labels = None\n",
    "        best_X_reduced = None\n",
    "        score_dict = {  \n",
    "            'norma_method': [],\n",
    "            'dim_red_method': [],\n",
    "            'dim_red_n_component': [],\n",
    "            'clustering_method': [],\n",
    "            'clustering_n_clusters': [],\n",
    "            'silhouette_score': [],\n",
    "            'DB_score': []\n",
    "        }\n",
    "\n",
    "        # Create a list of all parameter combinations for parallel processing\n",
    "        param_combinations = [(scaling_method, dim_red_method, cluster_method)\n",
    "                              for cluster_method in clustering_methods\n",
    "                              for scaling_method in scaling_methods\n",
    "                              for dim_red_method in dim_reduction_methods\n",
    "                             ]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Use multiprocessing to evaluate each combination\n",
    "        print(f\"CPU count: {mp.cpu_count}\")\n",
    "        # with mp.Pool(mp.cpu_count()-3) as p:\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "        # with mp.Pool(mp.cpu_count()-3) as p:\n",
    "            # results = p.map(self.clustering_worker, param_combinations)\n",
    "            # results = p.map(self.clustering_worker, param_combinations)\n",
    "            # # Filter out None results (when no clusters were generated)\n",
    "            # results = [result for result in results if result is not None]\n",
    "\n",
    "            # futures = {\n",
    "            #     executor.submit(self.clustering_worker,param_combinations[0]):param_combinations[0],\n",
    "            #     executor.submit(self.clustering_worker,param_combinations[1]):param_combinations[1],\n",
    "            #     executor.submit(self.clustering_worker,param_combinations[2]):param_combinations[2],\n",
    "            #     executor.submit(self.clustering_worker,param_combinations[3]):param_combinations[3],\n",
    "            # }\n",
    "            futures = {executor.submit(self.clustering_worker,i):i for i in param_combinations}\n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                \n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,best_n_cluster = result\n",
    "                        score_dict['norma_method'].append(scaling_method)\n",
    "                        score_dict['dim_red_method'].append(dim_red_method)\n",
    "                        score_dict['dim_red_n_component'].append(self.reduce_dim_n_component_dict[scaling_method][dim_red_method])\n",
    "                        score_dict['clustering_method'].append(cluster_method)\n",
    "                        score_dict['clustering_n_clusters'].append(best_n_cluster)\n",
    "                        score_dict['silhouette_score'].append(silhouette)\n",
    "                        score_dict['DB_score'].append(davies_bouldin)\n",
    "                #     print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n",
    "    \n",
    "            # # Process the results\n",
    "            # # for result in results.get():\n",
    "            # for result in  p.map(self.clustering_worker, param_combinations):\n",
    "            #     # print(result)\n",
    "            #     if result is not None:\n",
    "            #         scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,best_n_cluster = result\n",
    "        \n",
    "            #         score_dict['norma_method'].append(scaling_method)\n",
    "            #         score_dict['dim_red_method'].append(dim_red_method)\n",
    "            #         score_dict['dim_red_n_component'].append(self.reduced_data_dict[scaling_method][dim_red_method]['n_components'])\n",
    "            #         score_dict['clustering_method'].append(cluster_method)\n",
    "            #         score_dict['clustering_n_clusters'].append(best_n_cluster)\n",
    "            #         score_dict['silhouette_score'].append(silhouette)\n",
    "            #         score_dict['DB_score'].append(davies_bouldin)\n",
    "            \n",
    "        self.score_dict = score_dict \n",
    "        \n",
    "    \n",
    "    \n",
    "#     def compare_models(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "#         best_score = -float('inf')\n",
    "#         best_dim_red_method = None\n",
    "#         best_clustering_method = None\n",
    "#         best_labels = None\n",
    "#         best_X_reduced = None\n",
    "#         score_dict = {  'norma_method':[],\n",
    "#                         'dim_red_method':[],\n",
    "#                         'dim_red_n_component':[],\n",
    "#                         'clustering_method':[],\n",
    "#                         'clustering_n_clusters':[],\n",
    "#                         'silhouette_score':[],\n",
    "#                         'DB_score':[]}\n",
    "#         for scaling_method in scaling_methods:\n",
    "#             print('-'*50)\n",
    "#             print(scaling_method)\n",
    "#             # self.scaling_data(scaling_method)\n",
    "\n",
    "#             for dim_red_method in dim_reduction_methods:\n",
    "#                 print('*' * 50)\n",
    "#                 print(f' --{dim_red_method}')\n",
    "\n",
    "#                 # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "#                 self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "#                 self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "#                 for cluster_method in clustering_methods:\n",
    "#                     print(f'  ** Dimensionality Reduction: {dim_red_method}, Clustering: {cluster_method}')\n",
    "\n",
    "#                     self.clustering_data(cluster_method=cluster_method)\n",
    "#                     if len(set(self.labels)) >1:\n",
    "#                         silhouette, davies_bouldin= self.evaluate_clustering()\n",
    "\n",
    "# #                         print(f'Silhouette Score: {silhouette}')\n",
    "# #                         print(f'Davies-Bouldin Index: {davies_bouldin}')\n",
    "\n",
    "\n",
    "#                         score_dict['norma_method'].append(scaling_method)\n",
    "#                         score_dict['dim_red_method'].append(dim_red_method)\n",
    "#                         score_dict['dim_red_n_component'].append(self.dim_red_n_component)\n",
    "#                         score_dict['clustering_method'].append(cluster_method)\n",
    "#                         score_dict['clustering_n_clusters'].append(self.best_n_cluster)\n",
    "#                         score_dict['silhouette_score'].append(silhouette)\n",
    "#                         score_dict['DB_score'].append(davies_bouldin)\n",
    "\n",
    "\n",
    "\n",
    "#                         # Track the best configuration\n",
    "#                         score = silhouette  # Choose the metric to maximize, here we use silhouette score\n",
    "#                         if score > best_score:\n",
    "#                             best_score = score\n",
    "#                             best_dim_red_method = dim_red_method\n",
    "#                             best_clustering_method = cluster_method\n",
    "#                             best_labels = self.labels\n",
    "#                             best_X_reduced = self.reduced_df\n",
    "#                     else:\n",
    "#                         print('Unable to generate clusters')\n",
    "                    \n",
    "                    \n",
    "#         return best_dim_red_method, best_clustering_method, best_X_reduced, best_labels,score_dict\n",
    "    \n",
    "    def plot_clusters(self, X_reduced, labels, title='Cluster Plot'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, s=50, cmap='viridis')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9526dc37-b225-4c4c-8521-1a3ae43510a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU count: {mp.cpu_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7402af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa1b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f29db9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Data Loaded\n",
      "--------------------------------------------------\n",
      "Entering Data preprocessing \n",
      "(3900, 19)\n",
      "(3900, 18)\n",
      "--Column transformation\n",
      "** KNN imputer for missing values\n",
      "** One hot encoding for the Categorical features with less than 4 unique values\n",
      "** Ordinal encoding for the Categorical features with greater than 4 unique values\n",
      "--Feature Removing\n",
      "** Removing Constant features with 0 std\n",
      "** Removing Multicolinear features\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Define dimensionality reduction and clustering methods\n",
    "scaling_methods = ['normalization', 'standardization']\n",
    "\n",
    "dim_reduction_methods = ['PCA','UMAP','TSNE']\n",
    "\n",
    "# clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'Spectral']\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative','Hdbscan']\n",
    "\n",
    "\n",
    "# Create an instance of the class\n",
    "# Load and preprocess data\n",
    "model_selector = best_clustering_model_selector(df)\n",
    "# # Compare models\n",
    "\n",
    "# model_selector.prepare_reduced_data(scaling_methods,dim_reduction_methods)\n",
    "# best_dim_red, best_clustering, best_X_reduced, best_labels,score_dict = model_selector.compare_models(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "# model_selector.compare_models_parallel(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15199d69-38bd-4fbf-87d4-0a4cfd91f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00122909943262736\n"
     ]
    }
   ],
   "source": [
    "print((end-start )/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fd45f-66e5-404c-8ace-34606398533c",
   "metadata": {},
   "source": [
    "## Clustering model selection using Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "438bb70e-4cd5-4d9a-a4e2-69f9515bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24a3eeb3-aec8-4b1e-9c3c-3cacf9d5d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Scaling Numerical features\n",
      "** Scaling Numerical features\n",
      "  ** Selecting best N component for PCA\n",
      "  ** Selecting best N component for PCA\n",
      "  ** Selecting best N component for UMAP  ** Selecting best N component for UMAP\n",
      "  ** Selecting best N component for TSNE  ** best N component: 3  ** Selecting best N component for TSNEumap 2\n",
      "\n",
      "\n",
      "\n",
      "TSNE 2\n",
      "umap 2\n",
      "\n",
      "TSNE 2  ** best N component: 3\n",
      "\n",
      "('normalization', 'PCA') result: 3\n",
      "('standardization', 'PCA') result: 3\n",
      "TSNE 3\n",
      "umap 3\n",
      "TSNE 3\n",
      "umap 3\n",
      "umap 4\n",
      "umap 4\n",
      "umap 5\n",
      "umap 5\n",
      "umap 6\n",
      "umap 6\n",
      "  ** best N component for TSNE: 2\n",
      "('normalization', 'TSNE') result: 2\n",
      "  ** best N component for TSNE: 2\n",
      "('standardization', 'TSNE') result: 2\n",
      "  ** best N component for UMAP: 2\n",
      "('standardization', 'UMAP') result: 2\n",
      "  ** best N component for UMAP: 2\n",
      "('normalization', 'UMAP') result: 2\n",
      "2.7457951347033185\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_selector.reduce_data_parallel(scaling_methods,dim_reduction_methods)\n",
    "end = time.time()\n",
    "print((end-start )/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eab5112-810d-4f03-9363-b13170f6f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: <bound method BaseContext.cpu_count of <multiprocess.context.DefaultContext object at 0x7fd6466a2650>>\n",
      "** Scaling Numerical features\n",
      "** Scaling Numerical features\n",
      "** Scaling Numerical features\n",
      "** Scaling Numerical features\n",
      "** Scaling Numerical features** Scaling Numerical features** Scaling Numerical features\n",
      "\n",
      "\n",
      "** Scaling Numerical features\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "**************************************************4\n",
      "\n",
      "normalization5\n",
      "5 PCA DBSCAN\n",
      "model not created\n",
      "** Scaling Numerical features\n",
      "\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "65\n",
      "\n",
      "6\n",
      "6\n",
      "66\n",
      "\n",
      "77\n",
      "\n",
      "77\n",
      "\n",
      "77\n",
      "\n",
      "88\n",
      "\n",
      "88\n",
      "\n",
      "8\n",
      "9\n",
      "9\n",
      "8\n",
      "9\n",
      "99\n",
      "\n",
      "10\n",
      "10\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "completed\n",
      "KMeans model is not built\n",
      "**************************************************\n",
      "standardization10 \n",
      "completedPCA \n",
      "KMeans model is not builtKMeans\n",
      "\n",
      "model not created**************************************************\n",
      "\n",
      "normalization PCA KMeans\n",
      "** Scaling Numerical featuresmodel not created\n",
      "\n",
      "** Scaling Numerical features\n",
      "**************************************************\n",
      "standardization PCA DBSCAN\n",
      "model not created\n",
      "** Scaling Numerical features\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "completed\n",
      "**************************************************\n",
      "standardization TSNE DBSCAN\n",
      "model not created\n",
      "** Scaling Numerical features\n",
      "**************************************************\n",
      "standardization TSNE KMeans\n",
      "model Evaluation\n",
      "** Scaling Numerical features\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5**************************************************\n",
      "\n",
      "normalization TSNE 5\n",
      "DBSCAN6\n",
      "\n",
      "model not created6\n",
      "\n",
      "** Scaling Numerical features6\n",
      "\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "completed\n",
      "**************************************************\n",
      "normalization TSNE KMeans\n",
      "model Evaluation\n",
      "** Scaling Numerical features\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "completed\n",
      "**************************************************\n",
      "standardization UMAP KMeans\n",
      "model Evaluation\n",
      "** Scaling Numerical features\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "completed\n",
      "**************************************************\n",
      "normalization UMAP KMeans\n",
      "model Evaluation\n",
      "** Scaling Numerical features\n",
      "**************************************************\n",
      "standardization **************************************************\n",
      "normalizationUMAP DBSCAN\n",
      " PCAmodel not created Agglomerative\n",
      "model Evaluation\n",
      "\n",
      "** Scaling Numerical features** Scaling Numerical features\n",
      "\n",
      "** Scaling Numerical features\n",
      "('normalization', 'PCA', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "**************************************************\n",
      "normalization UMAP DBSCAN\n",
      "model not created\n",
      "** Scaling Numerical features\n",
      "** Scaling Numerical features\n",
      "('standardization', 'PCA', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "**************************************************\n",
      "standardization PCA Agglomerative\n",
      "model Evaluation\n",
      "** Scaling Numerical features\n",
      "('standardization', 'UMAP', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "('normalization', 'TSNE', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "**************************************************\n",
      "normalization TSNE Agglomerative\n",
      "model Evaluation\n",
      "('standardization', 'TSNE', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "**************************************************\n",
      "standardization UMAP Agglomerative\n",
      "model Evaluation\n",
      "**************************************************\n",
      "normalization UMAP Agglomerative\n",
      "model Evaluation\n",
      "('normalization', 'UMAP', 'Hdbscan') generated an exception: Unknown clustering method\n",
      "**************************************************\n",
      "standardization TSNE Agglomerative\n",
      "model Evaluation\n",
      "2.784569748242696\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# clustering_methods = ['KMeans']\n",
    "# clustering_methods = [ 'Agglomerative','DBSCAN']\n",
    "\n",
    "model_selector.compare_models_parallel(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "end = time.time()\n",
    "print((end-start )/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c8dd7-ae55-4759-bbc3-41bc8c8e73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector.score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "344b6452-c189-4bac-b8c5-9b13ec1b1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.DataFrame(model_selector.score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f9df91e-dfea-406b-8dfb-bc45992f8660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norma_method</th>\n",
       "      <th>dim_red_method</th>\n",
       "      <th>dim_red_n_component</th>\n",
       "      <th>clustering_method</th>\n",
       "      <th>clustering_n_clusters</th>\n",
       "      <th>silhouette_score</th>\n",
       "      <th>DB_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normalization</td>\n",
       "      <td>PCA</td>\n",
       "      <td>3</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.202324</td>\n",
       "      <td>1.511692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>standardization</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>2</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.202184</td>\n",
       "      <td>1.504791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normalization</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>2</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200993</td>\n",
       "      <td>1.392039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normalization</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>2</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>6</td>\n",
       "      <td>0.184998</td>\n",
       "      <td>1.607086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standardization</td>\n",
       "      <td>PCA</td>\n",
       "      <td>3</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.184866</td>\n",
       "      <td>1.637460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>normalization</td>\n",
       "      <td>TSNE</td>\n",
       "      <td>2</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.182387</td>\n",
       "      <td>1.519153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>standardization</td>\n",
       "      <td>TSNE</td>\n",
       "      <td>2</td>\n",
       "      <td>Agglomerative</td>\n",
       "      <td>4</td>\n",
       "      <td>0.169053</td>\n",
       "      <td>1.529502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standardization</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>2</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>7</td>\n",
       "      <td>0.165566</td>\n",
       "      <td>1.587075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standardization</td>\n",
       "      <td>TSNE</td>\n",
       "      <td>2</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>8</td>\n",
       "      <td>0.151134</td>\n",
       "      <td>1.820507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normalization</td>\n",
       "      <td>TSNE</td>\n",
       "      <td>2</td>\n",
       "      <td>KMeans</td>\n",
       "      <td>7</td>\n",
       "      <td>0.145948</td>\n",
       "      <td>1.605090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      norma_method dim_red_method  dim_red_n_component clustering_method  \\\n",
       "4    normalization            PCA                    3     Agglomerative   \n",
       "7  standardization           UMAP                    2     Agglomerative   \n",
       "8    normalization           UMAP                    2     Agglomerative   \n",
       "3    normalization           UMAP                    2            KMeans   \n",
       "5  standardization            PCA                    3     Agglomerative   \n",
       "6    normalization           TSNE                    2     Agglomerative   \n",
       "9  standardization           TSNE                    2     Agglomerative   \n",
       "2  standardization           UMAP                    2            KMeans   \n",
       "0  standardization           TSNE                    2            KMeans   \n",
       "1    normalization           TSNE                    2            KMeans   \n",
       "\n",
       "   clustering_n_clusters  silhouette_score  DB_score  \n",
       "4                      4          0.202324  1.511692  \n",
       "7                      4          0.202184  1.504791  \n",
       "8                      4          0.200993  1.392039  \n",
       "3                      6          0.184998  1.607086  \n",
       "5                      4          0.184866  1.637460  \n",
       "6                      4          0.182387  1.519153  \n",
       "9                      4          0.169053  1.529502  \n",
       "2                      7          0.165566  1.587075  \n",
       "0                      8          0.151134  1.820507  \n",
       "1                      7          0.145948  1.605090  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "615900d7-89c8-4acd-aa12-da3a957e36f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Data Loaded\n",
      "--------------------------------------------------\n",
      "Entering Data preprocessing \n",
      "(3900, 19)\n",
      "(3900, 18)\n",
      "--Column transformation\n",
      "** KNN imputer for missing values\n",
      "** One hot encoding for the Categorical features with less than 4 unique values\n",
      "** Ordinal encoding for the Categorical features with greater than 4 unique values\n",
      "--Feature Removing\n",
      "** Removing Constant features with 0 std\n",
      "** Removing Multicolinear features\n"
     ]
    }
   ],
   "source": [
    "val_model = best_clustering_model_selector(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d914ab9-47be-472d-bf35-c20f9a3a7644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Scaling Numerical features\n"
     ]
    }
   ],
   "source": [
    "val_model.scaling_data('normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0529da-7196-40cb-bebb-f1dba9b0c989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d6421-12bb-4009-9e9f-309ae4f37c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059deab-fe36-4646-8e82-6bca261ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector.score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3174da1a-3c99-42ea-82a0-a7a316a233bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = val_model.dimension_reduction['PCA']\n",
    "reducer.n_components = 3\n",
    "reduced_df = reducer.fit_transform(val_model.scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd007376-4fd2-489b-8e41-028f1dca07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = val_model.clusters['Agglomerative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca77c2f7-a468-4956-bb4d-f0028f9dce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.n_clusters = 4\n",
    "labels = clusterer.fit_predict(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bba4838-f90f-4abc-a5b8-3d6da6628be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = silhouette_score(val_model.scaled_df, labels)\n",
    "davies_bouldin = davies_bouldin_score(val_model.scaled_df,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6729d847-c82d-46c2-a20e-937469ded27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20232436262995446 1.511691512399707\n"
     ]
    }
   ],
   "source": [
    "print(silhouette,davies_bouldin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08248035-9625-4c7b-8b9e-d0a4db77e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normal processing \n",
    "# best_dim_red, best_clustering, best_X_reduced, best_labels,score_dict = model_selector.compare_models(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "\n",
    "# print((end-start )/60)\n",
    "\n",
    "# d=pd.DataFrame(score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabfa9f-efa8-466d-bf90-3c236218d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best result\n",
    "print(f'Best Dimensionality Reduction Method: {best_dim_red}')\n",
    "print(f'Best Clustering Method: {best_clustering}')\n",
    "model_selector.plot_clusters(best_X_reduced, best_labels, title=f'Best Clustering Results: {best_dim_red} + {best_clustering}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6fe8c4d-b4fa-41a1-9cab-af1a76cbc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensionality reduction and clustering methods\n",
    "scaling_methods = ['normalization', 'standardization']\n",
    "\n",
    "dim_reduction_methods = ['PCA','UMAP','TSNE']\n",
    "\n",
    "# clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'Spectral']\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative','Hdbscan']\n",
    "param_combinations = [(scaling_method, dim_red_method, cluster_method)\n",
    "                              for cluster_method in clustering_methods\n",
    "                              for scaling_method in scaling_methods\n",
    "                              for dim_red_method in dim_reduction_methods]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9fb930-f6eb-4bc4-b45f-3aa888768bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a0dfb1f-a084-46bf-9b23-258394731cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d['normalization']['TSNE'] = 'qwda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71e7f2f8-c15c-4012-9eda-f74c61475a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalization': {'TSNE': 'qwda'}, 'standardization': {'TSNE': None}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f7a0684-0c73-4f8c-a771-3f7d5aa3f8c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (523265653.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    {scale:{dim: None} for scale in for dim in dim_reduction_methods  scaling_methods}\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{scale:{dim: None} for scale in for dim in dim_reduction_methods  scaling_methods}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b335c-dd60-485c-8c84-4cea0e166126",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e5a13-069c-4ea4-af85-1d4f71275a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "    a = {executor.submit('aa',i):i for i in param_combinations}\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2f841-8dd2-40cb-8c23-6456ce0a0d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424486c-6262-4abc-b752-341d519477bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36225004-e92e-4bc8-85ef-fe282616c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def m_n_components_optimizer(self,a):\n",
    "        return f'welcome {a}'\n",
    "    def n_n_components_optimizer(self,a):\n",
    "        return f'welcome {a}'\n",
    "    def o_n_components_optimizer(self,a):\n",
    "        return f'welcome {a}'\n",
    "\n",
    "    def opt(self):\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {\n",
    "                executor.submit(getattr(self,f'{dim.lower()}_n_components_optimizer'),scaling): dim for scaling,dim in d}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3791d15-0b42-4b69-b88e-6e898e48dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca7298-c2c0-4ab2-84a8-93baf24be22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fc89b-3bdd-4634-bcf7-e3f908bc663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red = ['M','N','O']\n",
    "scaling = ['norma','stantad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13bb22-8222-4a70-b056-d6850d6fb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0e544-5db8-46b8-b978-a772a8ab4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff888e-7d6a-4e43-9124-e0d3e7828441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {\n",
    "                executor.submit(getattr(a,f'{dim.lower()}_n_components_optimizer'),scaling): dim for scaling,dim in d}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2c374-2fda-4933-9c9c-e4bd17392552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionReductionOptimiser:\n",
    "    def __init__(self, data,scaling_methods,dim_reduction_methods):\n",
    "        self.data_dict = data\n",
    "        self.scaling_methods = scaling_methods\n",
    "        self.dim_reduction_methods = dim_reduction_methods\n",
    "        self.pca_threshold = 0.95\n",
    "        self.tsne_min_components = 2\n",
    "        self.tsne_max_components = 3\n",
    "        self.umap_min_components = 2\n",
    "        self.umap_max_components = 6\n",
    "        self.n_components_dict = {method:[] for method in methods}\n",
    "          \n",
    "    def pca_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for PCA')\n",
    "        # Initialize PCA\n",
    "        pca = PCA()\n",
    "\n",
    "        # Fit PCA\n",
    "        X_pca = pca.fit_transform(self.data_dict[scaling_method])\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        best_n_components = len(cumulative_variance[cumulative_variance <= self.pca_threshold])  # Select PCs covering 95% variance\n",
    "        print(f'  ** best N component: {best_n_components}')\n",
    "        return  best_n_components\n",
    "\n",
    "    def tsne_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "            print(f'TSNE {n_components}')\n",
    "            \n",
    "            tsne = TSNE(n_components=n_components)\n",
    "            transformed_data = tsne.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(transformed_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(transformed_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for TSNE: {best_n_components}')\n",
    "        return  best_n_components\n",
    "\n",
    "    def umap_n_components_optimizer(self,scaling_method):\n",
    "        print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "            print(f'umap {n_components}')\n",
    "            reducer = umap.UMAP(n_components=n_components)\n",
    "            reduced_data = reducer.fit_transform(self.data_dict[scaling_method])\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(reduced_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(reduced_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for UMAP: {best_n_components}')\n",
    "        return best_n_components\n",
    "    \n",
    "    def optimize_all(self):\n",
    "        func_list = [(scale,dim) for dim in self.dim_reduction_methods for scale in self.scaling_methods]\n",
    "        \n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {executor.submit(getattr(self,f'{dim.lower()}_n_components_optimizer'),scaling): dim for scaling,dim in func_list}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    self.n_components_dict[method] = result\n",
    "                    print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251a2a1-d3f8-41a9-8f9d-870c4d82beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = f'{\"PCA\".lower()}_n_components_optimizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641b80b-2a0b-45cb-9841-e85be9bde184",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7215b32-83d8-4ec7-b7f4-b61ca757d5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
