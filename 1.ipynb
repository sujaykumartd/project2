{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "# from tqdm import tqdm \n",
    "# import multiprocessing as mp\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "TSNE generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "UMAP generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DimensionReductionOptimiser:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.pca_threshold = 0.95\n",
    "        self.tsne_min_components = 2\n",
    "        self.tsne_max_components = 3\n",
    "        self.umap_min_components = 2\n",
    "        self.umap_max_components = 10\n",
    "          \n",
    "    def pca_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for PCA')\n",
    "        # Initialize PCA\n",
    "        pca = PCA()\n",
    "\n",
    "        # Fit PCA\n",
    "        X_pca = pca.fit_transform(self.data)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        best_n_components = len(cumulative_variance[cumulative_variance <= self.pca_threshold])  # Select PCs covering 95% variance\n",
    "        print(f'  ** best N component: {best_n_components}')\n",
    "        return 'PCA', best_n_components\n",
    "\n",
    "    def tsne_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "            tsne = TSNE(n_components=n_components)\n",
    "            transformed_data = tsne.fit_transform(self.data)\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(transformed_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(transformed_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for TSNE: {best_n_components}')\n",
    "        return 'TSNE', best_n_components\n",
    "\n",
    "    def umap_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "            reducer = umap.UMAP(n_components=n_components)\n",
    "            reduced_data = reducer.fit_transform(self.data)\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(reduced_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(reduced_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "\n",
    "        print(f'  ** best N component for UMAP: {best_n_components}')\n",
    "        return 'UMAP', best_n_components\n",
    "    \n",
    "    def optimize_all(self):\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Submit tasks to the executor\n",
    "            futures = {\n",
    "                executor.submit(self.pca_n_components_optimizer): 'PCA',\n",
    "                executor.submit(self.tsne_n_components_optimizer): 'TSNE',\n",
    "                executor.submit(self.umap_n_components_optimizer): 'UMAP'\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                method = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    print(f'{method} result: {result}')\n",
    "                except Exception as exc:\n",
    "                    print(f'{method} generated an exception: {exc}')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Replace this with your dataset\n",
    "    data = np.random.rand(100, 50)  # Example dataset with 100 samples and 50 features\n",
    "\n",
    "    # Initialize the optimizer class with data\n",
    "    optimiser = DimensionReductionOptimiser(data)\n",
    "\n",
    "    # Run optimizers in parallel\n",
    "    optimiser.optimize_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "# Class to fetch the best n_cluster parameter for the clustering models\n",
    "\n",
    "class clustering_model_optimizer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.min_clusters = 4\n",
    "        self.max_clusters = 10\n",
    "        \n",
    "        \n",
    "    def kmeans_n_cluster_optimizer(self):\n",
    "        print('  *** Entered the elbow_plot method of the KMeansClustering class')\n",
    "        wcss=[] # initializing an empty list\n",
    "        for i in range (self.min_clusters,self.max_clusters+1):\n",
    "            kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42) # initializing the KMeans object\n",
    "            kmeans.fit(self.data) # fitting the data to the KMeans Algorithm\n",
    "            wcss.append(kmeans.inertia_)\n",
    "        kn = KneeLocator(range(self.min_clusters,self.max_clusters+1), wcss, curve='convex', direction='decreasing')\n",
    "        self.best_N_cluster =  kn.knee    \n",
    "\n",
    "    def agglomerative_spectural_n_cluster_optimizer(self,model_name):\n",
    "        print(f\"selecting best n_cluster parameter for model:{model_name}\")\n",
    "        score_dict = {'n_clusters':[],\n",
    "                     'silhouette_score':[],\n",
    "                     'DB_score':[]}\n",
    "\n",
    "        for n_clusters in range( self.min_clusters, self.max_clusters + 1):\n",
    "            if model_name == 'agglomerative':\n",
    "                model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            elif model_name == 'spectral':\n",
    "                print(n_clusters)\n",
    "                model = SpectralClustering(n_clusters=n_clusters)\n",
    "            else:\n",
    "                raise ValueError('Unknown cluster model name')\n",
    "                \n",
    "            labels = model.fit_predict(self.data)\n",
    "            silhouette_score_ = silhouette_score(self.data, labels)\n",
    "            davies_bouldin_score_ = davies_bouldin_score(self.data, labels)\n",
    "            score_dict['n_clusters'].append(n_clusters)\n",
    "            score_dict['silhouette_score'].append(silhouette_score_)\n",
    "            score_dict['DB_score'].append(davies_bouldin_score_)\n",
    "        score_df = pd.DataFrame(score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])        \n",
    "        self.best_N_cluster = score_df.n_clusters[0]\n",
    "        \n",
    "    def agglomerative_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='agglomerative')\n",
    "    def spectral_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='spectral')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class best_clustering_model_selector:\n",
    "    def __init__(self, df):\n",
    "        print('-'*50)\n",
    "        print('Data Loaded')\n",
    "        self.best_n_cluster = None\n",
    "        self.data = df\n",
    "        self.scalers = {\n",
    "                        'normalization': MinMaxScaler(),\n",
    "                        'standardization': StandardScaler()\n",
    "                        }\n",
    "        self.dimension_reduction={\n",
    "                        'PCA': PCA(),\n",
    "                        'UMAP':umap.UMAP(),\n",
    "                        'TSNE': TSNE()\n",
    "                        }\n",
    "\n",
    "        self.clusters = {\n",
    "                        'KMeans':KMeans(random_state=42),\n",
    "                        'DBSCAN':DBSCAN(eps=0.5, min_samples=5),\n",
    "                        'Agglomerative':AgglomerativeClustering(),\n",
    "                        'Spectral':SpectralClustering(random_state=42)\n",
    "                        }\n",
    "        # self.feature_engineering()\n",
    "        \n",
    "        \n",
    "    def prepare_data_for_preprocessing(self):\n",
    "    # Build preprocessing pipeline for categorical features\n",
    "        print(self.data.shape)\n",
    "        self.data.set_index('Customer ID',inplace=True)\n",
    "        print(self.data.shape)\n",
    "        \n",
    "        one_hot_cat_col = []\n",
    "        ordinal_cat_col = []\n",
    "        # creating dummies\n",
    "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = self.data.select_dtypes(include=['object']).columns\n",
    "        for feature in categorical_features:\n",
    "            unique_values = self.data[feature].nunique()\n",
    "            if unique_values < 4:\n",
    "                one_hot_cat_col.append(feature)\n",
    "            else:\n",
    "                ordinal_cat_col.append(feature)\n",
    "        self.one_hot_cat_col = one_hot_cat_col\n",
    "        self.ordinal_cat_col = ordinal_cat_col\n",
    "        self.numerical_col = numerical_features\n",
    "        \n",
    "    def data_preprocess_pipeline(self):\n",
    "        # Build preprocessing pipeline for numerical features\n",
    "        print(\"** KNN imputer for missing values\")\n",
    "        numerical_pipeline = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        #     ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "        # Build preprocessing pipeline for categorical features\n",
    "        print(\"** One hot encoding for the Categorical features with less than 4 unique values\")\n",
    "        \n",
    "        onehot_categorical_pipeline = Pipeline(steps=[\n",
    "            ('onehot_encoding', OneHotEncoder( handle_unknown='ignore'))])\n",
    "        \n",
    "        print(\"** Ordinal encoding for the Categorical features with greater than 4 unique values\")\n",
    "\n",
    "        ordinal_categorical_pipeline = Pipeline(steps=[\n",
    "            ('ordinal_encoding', OrdinalEncoder())\n",
    "\n",
    "        ])\n",
    "        column_transform = ColumnTransformer([\n",
    "            ('numerical_columns', numerical_pipeline, self.numerical_col),\n",
    "            ('onehot_categorical_columns', onehot_categorical_pipeline, self.one_hot_cat_col),\n",
    "            ('ordinal_categorical_columns', ordinal_categorical_pipeline, self.ordinal_cat_col)   \n",
    "        ])\n",
    "        self.trans_df = pd.DataFrame(column_transform.fit_transform(self.data)) \n",
    "        one_hot_cols = list(onehot_categorical_pipeline.fit(self.data[self.one_hot_cat_col]).get_feature_names_out())   \n",
    "        trans_cols = list(self.numerical_col)+one_hot_cols+list(self.ordinal_cat_col)  \n",
    "        self.trans_df.columns = trans_cols \n",
    "        \n",
    "        \n",
    "    def remove_constant_multicolinear_feature(self):\n",
    "        ### Removing constant features\n",
    "        print('** Removing Constant features with 0 std')\n",
    "        std_df = self.trans_df.describe().T['std']\n",
    "        const_feature_list = list(std_df[std_df==0].index)\n",
    "        if len(const_feature_list) > 0:\n",
    "            self.trans_df.drop(const_feature_list,axis=1,inplace=True)\n",
    "\n",
    "        ### Reducing number of features using correlation matrix \n",
    "        print('** Removing Multicolinear features')\n",
    "        corr_mat = self.trans_df.corr().abs()\n",
    "        upper_cor_mat_df = corr_mat.where(np.triu(np.ones(corr_mat.shape),k=1).astype(bool))\n",
    "        drop_col = [col for col in upper_cor_mat_df.columns if any(upper_cor_mat_df[col] > 0.85)]\n",
    "\n",
    "        self.data.reset_index(inplace=True)\n",
    "        self.trans_df = pd.concat([self.data['Customer ID'],self.trans_df],axis=1)\n",
    "        self.trans_df.drop(drop_col,axis=1,inplace=True)\n",
    "#         self.trans_df.set_index('Customer ID',inplace=True)\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        print('-'*50)\n",
    "        print('Entering Data preprocessing ')\n",
    "        self.prepare_data_for_preprocessing()\n",
    "        print('--Column transformation')\n",
    "        self.data_preprocess_pipeline()\n",
    "        print('--Feature Removing')\n",
    "        self.remove_constant_multicolinear_feature()\n",
    "        \n",
    "    def scaling_data(self,scaling_method):\n",
    "        if scaling_method in self.scalers.keys():\n",
    "            # Build preprocessing pipeline for numerical features\n",
    "            print(\"** Scaling Numerical features\")\n",
    "            scaling_pipeline = Pipeline(steps=[('scaler', self.scalers[scaling_method])])\n",
    "            temp_df = self.trans_df.drop(self.numerical_col,axis=1)\n",
    "            column_transform = ColumnTransformer([('numerical_columns', scaling_pipeline, self.numerical_col)])\n",
    "            self.scaled_df = pd.DataFrame(column_transform.fit_transform(self.trans_df))\n",
    "            self.scaled_df.columns=self.numerical_col\n",
    "            self.scaled_df = pd.concat([temp_df,self.scaled_df],axis=1)\n",
    "            self.scaled_df.set_index('Customer ID',inplace=True)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown Data Scaling method\")\n",
    "            \n",
    "       \n",
    "    \n",
    "    def reduce_dimensions(self, dimension_method='PCA'):\n",
    "        if dimension_method in self.dimension_reduction.keys():\n",
    "\n",
    "            # Create an instance of the class\n",
    "            n_comp_selector = dimension_redution_optimiser(self.scaled_df)\n",
    "\n",
    "            # Construct the method name dynamically and call it\n",
    "            method_name = f'{dimension_method.lower()}_n_components_optimizer'\n",
    "            # Use getattr to call the method\n",
    "            method = getattr(n_comp_selector, method_name, None)\n",
    "            if method:\n",
    "                method()  # Call the dynamically selected method\n",
    "                self.dim_red_n_component = n_comp_selector.best_n_components\n",
    "                self.reducer = self.dimension_reduction[dimension_method]\n",
    "                self.reducer.n_components = self.dim_red_n_component\n",
    "            else:\n",
    "                raise ValueError(f\"{method} not found\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown dimensionality reduction method\")\n",
    "        \n",
    "        self.reduced_df = self.reducer.fit_transform(self.scaled_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def clustering_data(self, cluster_method='KMeans'):\n",
    "        if cluster_method in self.clusters.keys():\n",
    "            \n",
    "            if cluster_method != 'DBSCAN':\n",
    "                # Create an instance of the class\n",
    "                n_cluster_selector = clustering_model_optimizer(self.reduced_df)\n",
    "\n",
    "                # Construct the method name dynamically and call it\n",
    "                method_name = f'{cluster_method.lower()}_n_cluster_optimizer'\n",
    "                # Use getattr to call the method\n",
    "                method = getattr(n_cluster_selector, method_name, None)\n",
    "                if method:\n",
    "                    method()  # Call the dynamically selected method\n",
    "                    self.best_n_cluster = n_cluster_selector.best_N_cluster\n",
    "                    self.clusterer = self.clusters[cluster_method]\n",
    "                    self.clusterer.n_clusters = self.best_n_cluster\n",
    "                else:\n",
    "                    raise ValueError(f\"{method} not found\")\n",
    "            elif cluster_method == 'DBSCAN':\n",
    "                self.clusterer = self.clusters[cluster_method]\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(\"Unknown clustering method\")\n",
    "\n",
    "        if self.best_n_cluster != None or cluster_method == 'DBSCAN':\n",
    "            self.labels = self.clusterer.fit_predict(self.reduced_df)\n",
    "            print(f'fit predict for methof {cluster_method}')\n",
    "            \n",
    "        else:\n",
    "            self.labels = []\n",
    "        print(f'completed cluster_data function dor cluster method {cluster_method}')\n",
    "        return self.labels\n",
    "    \n",
    "    def evaluate_clustering(self):\n",
    "        silhouette = silhouette_score(self.scaled_df, self.labels)\n",
    "        davies_bouldin = davies_bouldin_score(self.scaled_df, self.labels)\n",
    "        return silhouette, davies_bouldin\n",
    "    \n",
    "    def prepare_reduced_data(self,scaling_methods,dim_reduction_methods):\n",
    "        reduce_data_dict ={}\n",
    "        reduced_data_dict={}\n",
    "        scaled_data_dict = {}\n",
    "        for scaling_method in scaling_methods:\n",
    "            print('-'*50)\n",
    "            print(scaling_method)\n",
    "            self.scaling_data(scaling_method)\n",
    "            scaled_data_dict[scaling_method]=self.scaled_df\n",
    "            result_dict = {}\n",
    "            for dim_red_method in dim_reduction_methods:\n",
    "                print('*' * 50)\n",
    "                print(f' --{dim_red_method}')\n",
    "                self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "                result_dict[dim_red_method]={'data':self.reduced_df,'n_components':self.reducer.n_components}\n",
    "#                 print(scaling,dim)\n",
    "            reduced_data_dict[scaling_method] = result_dict\n",
    "        print(\"Completed...\")\n",
    "        self.reduced_data_dict = reduced_data_dict\n",
    "        self.scaled_data_dict = scaled_data_dict\n",
    "\n",
    "\n",
    "        \n",
    "      #   With paraller processing \n",
    "\n",
    "    # Helper function to perform clustering in parallel\n",
    "    def clustering_worker(self, params):\n",
    "        scaling_method, dim_red_method, cluster_method = params\n",
    "        # self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "        print('*'*50)\n",
    "        print(scaling_method, dim_red_method, cluster_method)\n",
    "        self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "        self.clustering_data(cluster_method=cluster_method)\n",
    "        \n",
    "        if len(set(self.labels)) > 1:\n",
    "            silhouette, davies_bouldin = self.evaluate_clustering()\n",
    "            print('model Evaluation')\n",
    "            return (scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,self.best_n_cluster)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def compare_models_parallel(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "        best_score = -float('inf')\n",
    "        best_dim_red_method = None\n",
    "        best_clustering_method = None\n",
    "        best_labels = None\n",
    "        best_X_reduced = None\n",
    "        score_dict = {  \n",
    "            'norma_method': [],\n",
    "            'dim_red_method': [],\n",
    "            'dim_red_n_component': [],\n",
    "            'clustering_method': [],\n",
    "            'clustering_n_clusters': [],\n",
    "            'silhouette_score': [],\n",
    "            'DB_score': []\n",
    "        }\n",
    "\n",
    "        # Create a list of all parameter combinations for parallel processing\n",
    "        param_combinations = [(scaling_method, dim_red_method, cluster_method)\n",
    "                              for scaling_method in scaling_methods\n",
    "                              for dim_red_method in dim_reduction_methods\n",
    "                              for cluster_method in clustering_methods]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Use multiprocessing to evaluate each combination\n",
    "        print(f\"CPU count: {mp.cpu_count}\")\n",
    "        with mp.Pool(mp.cpu_count()-3) as p:\n",
    "            results = p.map(self.clustering_worker, param_combinations)\n",
    "\n",
    "        # Filter out None results (when no clusters were generated)\n",
    "        results = [result for result in results if result is not None]\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,best_n_cluster = result\n",
    "\n",
    "            score_dict['norma_method'].append(scaling_method)\n",
    "            score_dict['dim_red_method'].append(dim_red_method)\n",
    "            score_dict['dim_red_n_component'].append(self.reduced_data_dict[scaling_method][dim_red_method]['n_components'])\n",
    "            score_dict['clustering_method'].append(cluster_method)\n",
    "            score_dict['clustering_n_clusters'].append(best_n_cluster)\n",
    "            score_dict['silhouette_score'].append(silhouette)\n",
    "            score_dict['DB_score'].append(davies_bouldin)\n",
    "            \n",
    "        self.score_dict = score_dict \n",
    "        \n",
    "    \n",
    "    \n",
    "#     def compare_models(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "#         best_score = -float('inf')\n",
    "#         best_dim_red_method = None\n",
    "#         best_clustering_method = None\n",
    "#         best_labels = None\n",
    "#         best_X_reduced = None\n",
    "#         score_dict = {  'norma_method':[],\n",
    "#                         'dim_red_method':[],\n",
    "#                         'dim_red_n_component':[],\n",
    "#                         'clustering_method':[],\n",
    "#                         'clustering_n_clusters':[],\n",
    "#                         'silhouette_score':[],\n",
    "#                         'DB_score':[]}\n",
    "#         for scaling_method in scaling_methods:\n",
    "#             print('-'*50)\n",
    "#             print(scaling_method)\n",
    "#             # self.scaling_data(scaling_method)\n",
    "\n",
    "#             for dim_red_method in dim_reduction_methods:\n",
    "#                 print('*' * 50)\n",
    "#                 print(f' --{dim_red_method}')\n",
    "\n",
    "#                 # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "#                 self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "#                 self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "#                 for cluster_method in clustering_methods:\n",
    "#                     print(f'  ** Dimensionality Reduction: {dim_red_method}, Clustering: {cluster_method}')\n",
    "\n",
    "#                     self.clustering_data(cluster_method=cluster_method)\n",
    "#                     if len(set(self.labels)) >1:\n",
    "#                         silhouette, davies_bouldin= self.evaluate_clustering()\n",
    "\n",
    "# #                         print(f'Silhouette Score: {silhouette}')\n",
    "# #                         print(f'Davies-Bouldin Index: {davies_bouldin}')\n",
    "\n",
    "\n",
    "#                         score_dict['norma_method'].append(scaling_method)\n",
    "#                         score_dict['dim_red_method'].append(dim_red_method)\n",
    "#                         score_dict['dim_red_n_component'].append(self.dim_red_n_component)\n",
    "#                         score_dict['clustering_method'].append(cluster_method)\n",
    "#                         score_dict['clustering_n_clusters'].append(self.best_n_cluster)\n",
    "#                         score_dict['silhouette_score'].append(silhouette)\n",
    "#                         score_dict['DB_score'].append(davies_bouldin)\n",
    "\n",
    "\n",
    "\n",
    "#                         # Track the best configuration\n",
    "#                         score = silhouette  # Choose the metric to maximize, here we use silhouette score\n",
    "#                         if score > best_score:\n",
    "#                             best_score = score\n",
    "#                             best_dim_red_method = dim_red_method\n",
    "#                             best_clustering_method = cluster_method\n",
    "#                             best_labels = self.labels\n",
    "#                             best_X_reduced = self.reduced_df\n",
    "#                     else:\n",
    "#                         print('Unable to generate clusters')\n",
    "                    \n",
    "                    \n",
    "#         return best_dim_red_method, best_clustering_method, best_X_reduced, best_labels,score_dict\n",
    "    \n",
    "    def plot_clusters(self, X_reduced, labels, title='Cluster Plot'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, s=50, cmap='viridis')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensionality reduction and clustering methods\n",
    "scaling_methods = ['normalization', 'standardization']\n",
    "dim_reduction_methods = ['PCA', 'UMAP','TSNE']\n",
    "# dim_reduction_methods = ['PCA',]\n",
    "\n",
    "# clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'Spectral']\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative',]\n",
    "clustering_methods = ['KMeans', 'DBSCAN',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "model_selector = best_clustering_model_selector(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 19)\n",
      "(3900, 18)\n"
     ]
    }
   ],
   "source": [
    "model_selector.prepare_data_for_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** KNN imputer for missing values\n",
      "** One hot encoding for the Categorical features with less than 4 unique values\n",
      "** Ordinal encoding for the Categorical features with greater than 4 unique values\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d953ff6efa50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preprocess_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-0e4c7a93b245>\u001b[0m in \u001b[0;36mdata_preprocess_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         ])\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mone_hot_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_categorical_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_cat_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mtrans_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mone_hot_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordinal_cat_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "model_selector.data_preprocess_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Removing Constant features with 0 std\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'best_clustering_model_selector' object has no attribute 'trans_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4083d62b0525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_constant_multicolinear_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-0e4c7a93b245>\u001b[0m in \u001b[0;36mremove_constant_multicolinear_feature\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m### Removing constant features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'** Removing Constant features with 0 std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mstd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mconst_feature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstd_df\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_feature_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'best_clustering_model_selector' object has no attribute 'trans_df'"
     ]
    }
   ],
   "source": [
    "model_selector.remove_constant_multicolinear_feature()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
