{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb775c99-7432-4c9a-af42-9fff035d8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn \n",
    "# !pip3 install numpy==2.0\n",
    "# !pip install kneed \n",
    "# !pip install umap-learn\n",
    "# !pip install multiprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c770f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 12:43:47.372252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "# from tqdm import tqdm \n",
    "# import multiprocessing as mp\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6e7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de65064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction class\n",
    "class dimension_redution_optimiser:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.pca_threshold = 0.95\n",
    "        self.tsne_min_components = 2\n",
    "        self.tsne_max_components = 3\n",
    "        self.umap_min_components = 2\n",
    "        self.umap_max_components = 10\n",
    "          \n",
    "    def pca_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for PCA')\n",
    "        # Initialize PCA\n",
    "        pca = PCA()\n",
    "\n",
    "        # Fit PCA\n",
    "        X_pca = pca.fit_transform(self.data)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        self.best_n_components = len(cumulative_variance[cumulative_variance <=  self.pca_threshold]) #select PC covers 95variance\n",
    "        print(f'  ** best N component: {self.best_n_components}')\n",
    "         \n",
    "    \n",
    "\n",
    "    def tsne_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for TSNE')\n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.tsne_min_components, self.tsne_max_components + 1):\n",
    "            tsne = TSNE(n_components=n_components)\n",
    "            transformed_data = tsne.fit_transform(self.data)\n",
    "#             print(n_components)\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(transformed_data)\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            if(len(set(labels))>1):\n",
    "                score = silhouette_score(transformed_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "        self.best_n_components = best_n_components\n",
    "        print(f'  ** best N component: {self.best_n_components}')\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def umap_n_components_optimizer(self):\n",
    "        print('  ** Selecting best N component for UMAP')\n",
    "        \n",
    "        \n",
    "        best_n_components = None\n",
    "        best_score = -1\n",
    "\n",
    "        for n_components in range(self.umap_min_components, self.umap_max_components + 1):\n",
    "            print(n_components)\n",
    "            reducer = umap.UMAP(n_components=n_components)\n",
    "            reduced_data = reducer.fit_transform(self.data)\n",
    "            \n",
    "            # Example with KMeans clustering (can replace with other methods)\n",
    "            kmeans = KMeans(n_clusters=4)\n",
    "            kmeans.fit(reduced_data)\n",
    "            labels = kmeans.labels_\n",
    "            if(len(set(labels))>1):\n",
    "                score = silhouette_score(reduced_data, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n_components = n_components\n",
    "            print(n_components)\n",
    "            \n",
    "\n",
    "        self.best_n_components = best_n_components\n",
    "        print(f'  ** best N component: {self.best_n_components}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7d9ac-f1b3-4eb7-8ed2-c291b96e6482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6568c-2f3c-454d-bd5b-fd70e0b309a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5659e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "# Class to fetch the best n_cluster parameter for the clustering models\n",
    "\n",
    "class clustering_model_optimizer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.min_clusters = 4\n",
    "        self.max_clusters = 10\n",
    "        \n",
    "        \n",
    "    def kmeans_n_cluster_optimizer(self):\n",
    "        print('  *** Entered the elbow_plot method of the KMeansClustering class')\n",
    "        wcss=[] # initializing an empty list\n",
    "        for i in range (self.min_clusters,self.max_clusters+1):\n",
    "            kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42) # initializing the KMeans object\n",
    "            kmeans.fit(self.data) # fitting the data to the KMeans Algorithm\n",
    "            wcss.append(kmeans.inertia_)\n",
    "        kn = KneeLocator(range(self.min_clusters,self.max_clusters+1), wcss, curve='convex', direction='decreasing')\n",
    "        self.best_N_cluster =  kn.knee    \n",
    "\n",
    "    def agglomerative_spectural_n_cluster_optimizer(self,model_name):\n",
    "        print(f\"selecting best n_cluster parameter for model:{model_name}\")\n",
    "        score_dict = {'n_clusters':[],\n",
    "                     'silhouette_score':[],\n",
    "                     'DB_score':[]}\n",
    "\n",
    "        for n_clusters in range( self.min_clusters, self.max_clusters + 1):\n",
    "            if model_name == 'agglomerative':\n",
    "                model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            elif model_name == 'spectral':\n",
    "                print(n_clusters)\n",
    "                model = SpectralClustering(n_clusters=n_clusters)\n",
    "            else:\n",
    "                raise ValueError('Unknown cluster model name')\n",
    "                \n",
    "            labels = model.fit_predict(self.data)\n",
    "            silhouette_score_ = silhouette_score(self.data, labels)\n",
    "            davies_bouldin_score_ = davies_bouldin_score(self.data, labels)\n",
    "            score_dict['n_clusters'].append(n_clusters)\n",
    "            score_dict['silhouette_score'].append(silhouette_score_)\n",
    "            score_dict['DB_score'].append(davies_bouldin_score_)\n",
    "        score_df = pd.DataFrame(score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])        \n",
    "        self.best_N_cluster = score_df.n_clusters[0]\n",
    "        \n",
    "    def agglomerative_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='agglomerative')\n",
    "    def spectral_n_cluster_optimizer(self):\n",
    "        self.agglomerative_spectural_n_cluster_optimizer(model_name='spectral')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class best_clustering_model_selector:\n",
    "    def __init__(self, df):\n",
    "        print('-'*50)\n",
    "        print('Data Loaded')\n",
    "        self.best_n_cluster = None\n",
    "        self.data = df\n",
    "        self.scalers = {\n",
    "                        'normalization': MinMaxScaler(),\n",
    "                        'standardization': StandardScaler()\n",
    "                        }\n",
    "        self.dimension_reduction={\n",
    "                        'PCA': PCA(),\n",
    "                        'UMAP':umap.UMAP(),\n",
    "                        'TSNE': TSNE()\n",
    "                        }\n",
    "\n",
    "        self.clusters = {\n",
    "                        'KMeans':KMeans(random_state=42),\n",
    "                        'DBSCAN':DBSCAN(eps=0.5, min_samples=5),\n",
    "                        'Agglomerative':AgglomerativeClustering(),\n",
    "                        'Spectral':SpectralClustering(random_state=42)\n",
    "                        }\n",
    "        self.feature_engineering()\n",
    "        \n",
    "        \n",
    "    def prepare_data_for_preprocessing(self):\n",
    "    # Build preprocessing pipeline for categorical features\n",
    "        print(self.data.shape)\n",
    "        self.data.set_index('Customer ID',inplace=True)\n",
    "        print(self.data.shape)\n",
    "        \n",
    "        one_hot_cat_col = []\n",
    "        ordinal_cat_col = []\n",
    "        # creating dummies\n",
    "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = self.data.select_dtypes(include=['object']).columns\n",
    "        for feature in categorical_features:\n",
    "            unique_values = self.data[feature].nunique()\n",
    "            if unique_values < 4:\n",
    "                one_hot_cat_col.append(feature)\n",
    "            else:\n",
    "                ordinal_cat_col.append(feature)\n",
    "        self.one_hot_cat_col = one_hot_cat_col\n",
    "        self.ordinal_cat_col = ordinal_cat_col\n",
    "        self.numerical_col = numerical_features\n",
    "        \n",
    "    def data_preprocess_pipeline(self):\n",
    "        # Build preprocessing pipeline for numerical features\n",
    "        print(\"** KNN imputer for missing values\")\n",
    "        numerical_pipeline = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        #     ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "        # Build preprocessing pipeline for categorical features\n",
    "        print(\"** One hot encoding for the Categorical features with less than 4 unique values\")\n",
    "        \n",
    "        onehot_categorical_pipeline = Pipeline(steps=[\n",
    "            ('onehot_encoding', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "        \n",
    "        print(\"** Ordinal encoding for the Categorical features with greater than 4 unique values\")\n",
    "\n",
    "        ordinal_categorical_pipeline = Pipeline(steps=[\n",
    "            ('ordinal_encoding', OrdinalEncoder())\n",
    "\n",
    "        ])\n",
    "        column_transform = ColumnTransformer([\n",
    "            ('numerical_columns', numerical_pipeline, self.numerical_col),\n",
    "            ('onehot_categorical_columns', onehot_categorical_pipeline, self.one_hot_cat_col),\n",
    "            ('ordinal_categorical_columns', ordinal_categorical_pipeline, self.ordinal_cat_col)   \n",
    "        ])\n",
    "        self.trans_df = pd.DataFrame(column_transform.fit_transform(self.data)) \n",
    "        one_hot_cols = list(onehot_categorical_pipeline.fit(self.data[self.one_hot_cat_col]).get_feature_names_out())   \n",
    "        trans_cols = list(self.numerical_col)+one_hot_cols+list(self.ordinal_cat_col)  \n",
    "        self.trans_df.columns = trans_cols \n",
    "        \n",
    "        \n",
    "    def remove_constant_multicolinear_feature(self):\n",
    "        ### Removing constant features\n",
    "        print('** Removing Constant features with 0 std')\n",
    "        std_df = self.trans_df.describe().T['std']\n",
    "        const_feature_list = list(std_df[std_df==0].index)\n",
    "        if len(const_feature_list) > 0:\n",
    "            self.trans_df.drop(const_feature_list,axis=1,inplace=True)\n",
    "\n",
    "        ### Reducing number of features using correlation matrix \n",
    "        print('** Removing Multicolinear features')\n",
    "        corr_mat = self.trans_df.corr().abs()\n",
    "        upper_cor_mat_df = corr_mat.where(np.triu(np.ones(corr_mat.shape),k=1).astype(bool))\n",
    "        drop_col = [col for col in upper_cor_mat_df.columns if any(upper_cor_mat_df[col] > 0.85)]\n",
    "\n",
    "        self.data.reset_index(inplace=True)\n",
    "        self.trans_df = pd.concat([self.data['Customer ID'],self.trans_df],axis=1)\n",
    "        self.trans_df.drop(drop_col,axis=1,inplace=True)\n",
    "#         self.trans_df.set_index('Customer ID',inplace=True)\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        print('-'*50)\n",
    "        print('Entering Data preprocessing ')\n",
    "        self.prepare_data_for_preprocessing()\n",
    "        print('--Column transformation')\n",
    "        self.data_preprocess_pipeline()\n",
    "        print('--Feature Removing')\n",
    "        self.remove_constant_multicolinear_feature()\n",
    "        \n",
    "    def scaling_data(self,scaling_method):\n",
    "        if scaling_method in self.scalers.keys():\n",
    "            # Build preprocessing pipeline for numerical features\n",
    "            print(\"** Scaling Numerical features\")\n",
    "            scaling_pipeline = Pipeline(steps=[('scaler', self.scalers[scaling_method])])\n",
    "            temp_df = self.trans_df.drop(self.numerical_col,axis=1)\n",
    "            column_transform = ColumnTransformer([('numerical_columns', scaling_pipeline, self.numerical_col)])\n",
    "            self.scaled_df = pd.DataFrame(column_transform.fit_transform(self.trans_df))\n",
    "            self.scaled_df.columns=self.numerical_col\n",
    "            self.scaled_df = pd.concat([temp_df,self.scaled_df],axis=1)\n",
    "            self.scaled_df.set_index('Customer ID',inplace=True)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown Data Scaling method\")\n",
    "            \n",
    "       \n",
    "    \n",
    "    def reduce_dimensions(self, dimension_method='PCA'):\n",
    "        if dimension_method in self.dimension_reduction.keys():\n",
    "\n",
    "            # Create an instance of the class\n",
    "            n_comp_selector = dimension_redution_optimiser(self.scaled_df)\n",
    "\n",
    "            # Construct the method name dynamically and call it\n",
    "            method_name = f'{dimension_method.lower()}_n_components_optimizer'\n",
    "            # Use getattr to call the method\n",
    "            method = getattr(n_comp_selector, method_name, None)\n",
    "            if method:\n",
    "                method()  # Call the dynamically selected method\n",
    "                self.dim_red_n_component = n_comp_selector.best_n_components\n",
    "                self.reducer = self.dimension_reduction[dimension_method]\n",
    "                self.reducer.n_components = self.dim_red_n_component\n",
    "            else:\n",
    "                raise ValueError(f\"{method} not found\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown dimensionality reduction method\")\n",
    "        \n",
    "        self.reduced_df = self.reducer.fit_transform(self.scaled_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def clustering_data(self, cluster_method='KMeans'):\n",
    "        if cluster_method in self.clusters.keys():\n",
    "            \n",
    "            if cluster_method != 'DBSCAN':\n",
    "                # Create an instance of the class\n",
    "                n_cluster_selector = clustering_model_optimizer(self.reduced_df)\n",
    "\n",
    "                # Construct the method name dynamically and call it\n",
    "                method_name = f'{cluster_method.lower()}_n_cluster_optimizer'\n",
    "                # Use getattr to call the method\n",
    "                method = getattr(n_cluster_selector, method_name, None)\n",
    "                if method:\n",
    "                    method()  # Call the dynamically selected method\n",
    "                    self.best_n_cluster = n_cluster_selector.best_N_cluster\n",
    "                    self.clusterer = self.clusters[cluster_method]\n",
    "                    self.clusterer.n_clusters = self.best_n_cluster\n",
    "                else:\n",
    "                    raise ValueError(f\"{method} not found\")\n",
    "            elif cluster_method == 'DBSCAN':\n",
    "                self.clusterer = self.clusters[cluster_method]\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(\"Unknown clustering method\")\n",
    "\n",
    "        if self.best_n_cluster != None or cluster_method == 'DBSCAN':\n",
    "            self.labels = self.clusterer.fit_predict(self.reduced_df)\n",
    "            print(f'fit predict for methof {cluster_method}')\n",
    "            \n",
    "        else:\n",
    "            self.labels = []\n",
    "        print(f'completed cluster_data function dor cluster method {cluster_method}')\n",
    "        return self.labels\n",
    "    \n",
    "    def evaluate_clustering(self):\n",
    "        silhouette = silhouette_score(self.scaled_df, self.labels)\n",
    "        davies_bouldin = davies_bouldin_score(self.scaled_df, self.labels)\n",
    "        return silhouette, davies_bouldin\n",
    "    \n",
    "    def prepare_reduced_data(self,scaling_methods,dim_reduction_methods):\n",
    "        reduce_data_dict ={}\n",
    "        reduced_data_dict={}\n",
    "        scaled_data_dict = {}\n",
    "        for scaling_method in scaling_methods:\n",
    "            print('-'*50)\n",
    "            print(scaling_method)\n",
    "            self.scaling_data(scaling_method)\n",
    "            scaled_data_dict[scaling_method]=self.scaled_df\n",
    "            result_dict = {}\n",
    "            for dim_red_method in dim_reduction_methods:\n",
    "                print('*' * 50)\n",
    "                print(f' --{dim_red_method}')\n",
    "                self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "                result_dict[dim_red_method]={'data':self.reduced_df,'n_components':self.reducer.n_components}\n",
    "#                 print(scaling,dim)\n",
    "            reduced_data_dict[scaling_method] = result_dict\n",
    "        print(\"Completed...\")\n",
    "        self.reduced_data_dict = reduced_data_dict\n",
    "        self.scaled_data_dict = scaled_data_dict\n",
    "\n",
    "\n",
    "        \n",
    "      #   With paraller processing \n",
    "\n",
    "    # Helper function to perform clustering in parallel\n",
    "    def clustering_worker(self, params):\n",
    "        scaling_method, dim_red_method, cluster_method = params\n",
    "        # self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "        print('*'*50)\n",
    "        print(scaling_method, dim_red_method, cluster_method)\n",
    "        self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "        self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "        self.clustering_data(cluster_method=cluster_method)\n",
    "        \n",
    "        if len(set(self.labels)) > 1:\n",
    "            silhouette, davies_bouldin = self.evaluate_clustering()\n",
    "            print('model Evaluation')\n",
    "            return (scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,self.best_n_cluster)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def compare_models_parallel(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "        best_score = -float('inf')\n",
    "        best_dim_red_method = None\n",
    "        best_clustering_method = None\n",
    "        best_labels = None\n",
    "        best_X_reduced = None\n",
    "        score_dict = {  \n",
    "            'norma_method': [],\n",
    "            'dim_red_method': [],\n",
    "            'dim_red_n_component': [],\n",
    "            'clustering_method': [],\n",
    "            'clustering_n_clusters': [],\n",
    "            'silhouette_score': [],\n",
    "            'DB_score': []\n",
    "        }\n",
    "\n",
    "        # Create a list of all parameter combinations for parallel processing\n",
    "        param_combinations = [(scaling_method, dim_red_method, cluster_method)\n",
    "                              for scaling_method in scaling_methods\n",
    "                              for dim_red_method in dim_reduction_methods\n",
    "                              for cluster_method in clustering_methods]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Use multiprocessing to evaluate each combination\n",
    "        print(f\"CPU count: {mp.cpu_count}\")\n",
    "        with mp.Pool(mp.cpu_count()-3) as p:\n",
    "            results = p.map(self.clustering_worker, param_combinations)\n",
    "\n",
    "        # Filter out None results (when no clusters were generated)\n",
    "        results = [result for result in results if result is not None]\n",
    "\n",
    "        # Process the results\n",
    "        for result in results:\n",
    "            scaling_method, dim_red_method, cluster_method, silhouette, davies_bouldin,best_n_cluster = result\n",
    "\n",
    "            score_dict['norma_method'].append(scaling_method)\n",
    "            score_dict['dim_red_method'].append(dim_red_method)\n",
    "            score_dict['dim_red_n_component'].append(self.reduced_data_dict[scaling_method][dim_red_method]['n_components'])\n",
    "            score_dict['clustering_method'].append(cluster_method)\n",
    "            score_dict['clustering_n_clusters'].append(best_n_cluster)\n",
    "            score_dict['silhouette_score'].append(silhouette)\n",
    "            score_dict['DB_score'].append(davies_bouldin)\n",
    "            \n",
    "        self.score_dict = score_dict \n",
    "        \n",
    "    \n",
    "    \n",
    "#     def compare_models(self, scaling_methods, dim_reduction_methods, clustering_methods):\n",
    "#         best_score = -float('inf')\n",
    "#         best_dim_red_method = None\n",
    "#         best_clustering_method = None\n",
    "#         best_labels = None\n",
    "#         best_X_reduced = None\n",
    "#         score_dict = {  'norma_method':[],\n",
    "#                         'dim_red_method':[],\n",
    "#                         'dim_red_n_component':[],\n",
    "#                         'clustering_method':[],\n",
    "#                         'clustering_n_clusters':[],\n",
    "#                         'silhouette_score':[],\n",
    "#                         'DB_score':[]}\n",
    "#         for scaling_method in scaling_methods:\n",
    "#             print('-'*50)\n",
    "#             print(scaling_method)\n",
    "#             # self.scaling_data(scaling_method)\n",
    "\n",
    "#             for dim_red_method in dim_reduction_methods:\n",
    "#                 print('*' * 50)\n",
    "#                 print(f' --{dim_red_method}')\n",
    "\n",
    "#                 # self.reduce_dimensions(dimension_method=dim_red_method)\n",
    "#                 self.reduced_df = self.reduced_data_dict[scaling_method][dim_red_method]['data']\n",
    "#                 self.scaled_df = self.scaled_data_dict[scaling_method]\n",
    "#                 for cluster_method in clustering_methods:\n",
    "#                     print(f'  ** Dimensionality Reduction: {dim_red_method}, Clustering: {cluster_method}')\n",
    "\n",
    "#                     self.clustering_data(cluster_method=cluster_method)\n",
    "#                     if len(set(self.labels)) >1:\n",
    "#                         silhouette, davies_bouldin= self.evaluate_clustering()\n",
    "\n",
    "# #                         print(f'Silhouette Score: {silhouette}')\n",
    "# #                         print(f'Davies-Bouldin Index: {davies_bouldin}')\n",
    "\n",
    "\n",
    "#                         score_dict['norma_method'].append(scaling_method)\n",
    "#                         score_dict['dim_red_method'].append(dim_red_method)\n",
    "#                         score_dict['dim_red_n_component'].append(self.dim_red_n_component)\n",
    "#                         score_dict['clustering_method'].append(cluster_method)\n",
    "#                         score_dict['clustering_n_clusters'].append(self.best_n_cluster)\n",
    "#                         score_dict['silhouette_score'].append(silhouette)\n",
    "#                         score_dict['DB_score'].append(davies_bouldin)\n",
    "\n",
    "\n",
    "\n",
    "#                         # Track the best configuration\n",
    "#                         score = silhouette  # Choose the metric to maximize, here we use silhouette score\n",
    "#                         if score > best_score:\n",
    "#                             best_score = score\n",
    "#                             best_dim_red_method = dim_red_method\n",
    "#                             best_clustering_method = cluster_method\n",
    "#                             best_labels = self.labels\n",
    "#                             best_X_reduced = self.reduced_df\n",
    "#                     else:\n",
    "#                         print('Unable to generate clusters')\n",
    "                    \n",
    "                    \n",
    "#         return best_dim_red_method, best_clustering_method, best_X_reduced, best_labels,score_dict\n",
    "    \n",
    "    def plot_clusters(self, X_reduced, labels, title='Cluster Plot'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, s=50, cmap='viridis')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7402af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa1b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f29db9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Data Loaded\n",
      "--------------------------------------------------\n",
      "Entering Data preprocessing \n",
      "(3900, 19)\n",
      "(3900, 18)\n",
      "--Column transformation\n",
      "** KNN imputer for missing values\n",
      "** One hot encoding for the Categorical features with less than 4 unique values\n",
      "** Ordinal encoding for the Categorical features with greater than 4 unique values\n",
      "--Feature Removing\n",
      "** Removing Constant features with 0 std\n",
      "** Removing Multicolinear features\n",
      "--------------------------------------------------\n",
      "normalization\n",
      "** Scaling Numerical features\n",
      "**************************************************\n",
      " --PCA\n",
      "  ** Selecting best N component for PCA\n",
      "  ** best N component: 3\n",
      "**************************************************\n",
      " --UMAP\n",
      "  ** Selecting best N component for UMAP\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "  ** best N component: 2\n",
      "**************************************************\n",
      " --TSNE\n",
      "  ** Selecting best N component for TSNE\n",
      "  ** best N component: 2\n",
      "--------------------------------------------------\n",
      "standardization\n",
      "** Scaling Numerical features\n",
      "**************************************************\n",
      " --PCA\n",
      "  ** Selecting best N component for PCA\n",
      "  ** best N component: 3\n",
      "**************************************************\n",
      " --UMAP\n",
      "  ** Selecting best N component for UMAP\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "  ** best N component: 2\n",
      "**************************************************\n",
      " --TSNE\n",
      "  ** Selecting best N component for TSNE\n",
      "  ** best N component: 2\n",
      "Completed...\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Define dimensionality reduction and clustering methods\n",
    "scaling_methods = ['normalization', 'standardization']\n",
    "dim_reduction_methods = ['PCA', 'UMAP','TSNE']\n",
    "# dim_reduction_methods = ['PCA',]\n",
    "\n",
    "# clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'Spectral']\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative',]\n",
    "clustering_methods = ['KMeans', 'DBSCAN',]\n",
    "\n",
    "# clustering_methods = ['DBSCAN',]\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the class\n",
    "# Load and preprocess data\n",
    "model_selector = best_clustering_model_selector(df)\n",
    "# Compare models\n",
    "\n",
    "model_selector.prepare_reduced_data(scaling_methods,dim_reduction_methods)\n",
    "# best_dim_red, best_clustering, best_X_reduced, best_labels,score_dict = model_selector.compare_models(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "# model_selector.compare_models_parallel(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15199d69-38bd-4fbf-87d4-0a4cfd91f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.419387491544088\n"
     ]
    }
   ],
   "source": [
    "print((end-start )/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fd45f-66e5-404c-8ace-34606398533c",
   "metadata": {},
   "source": [
    "## Clustering model selection using Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "438bb70e-4cd5-4d9a-a4e2-69f9515bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff82ab-d6a1-4bf3-badd-7be2cd3901a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: <bound method BaseContext.cpu_count of <multiprocess.context.DefaultContext object at 0x7f10b94fce80>>\n",
      "**************************************************\n",
      "normalization PCA KMeans\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "**************************************************\n",
      "normalization PCA DBSCAN\n",
      "fit predict for methof DBSCAN\n",
      "completed cluster_data function dor cluster method DBSCAN\n",
      "**************************************************\n",
      "standardization PCA KMeans\n",
      "  *** Entered the elbow_plot method of the KMeansClustering class\n",
      "**************************************************\n",
      "standardization PCA DBSCAN\n",
      "fit predict for methof DBSCAN\n",
      "completed cluster_data function dor cluster method DBSCAN\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dim_reduction_methods = ['PCA',]\n",
    "\n",
    "clustering_methods = ['KMeans','DBSCAN' ]\n",
    "model_selector.compare_models_parallel(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "end = time.time()\n",
    "print((end-start )/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c8dd7-ae55-4759-bbc3-41bc8c8e73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((end-start )/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b6452-c189-4bac-b8c5-9b13ec1b1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.DataFrame(model_selector.score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174da1a-3c99-42ea-82a0-a7a316a233bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (end-start )/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08248035-9625-4c7b-8b9e-d0a4db77e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normal processing \n",
    "# best_dim_red, best_clustering, best_X_reduced, best_labels,score_dict = model_selector.compare_models(scaling_methods,dim_reduction_methods, clustering_methods)\n",
    "\n",
    "# print((end-start )/60)\n",
    "\n",
    "# d=pd.DataFrame(score_dict).sort_values(['silhouette_score','DB_score'],ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabfa9f-efa8-466d-bf90-3c236218d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best result\n",
    "print(f'Best Dimensionality Reduction Method: {best_dim_red}')\n",
    "print(f'Best Clustering Method: {best_clustering}')\n",
    "model_selector.plot_clusters(best_X_reduced, best_labels, title=f'Best Clustering Results: {best_dim_red} + {best_clustering}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe8c4d-b4fa-41a1-9cab-af1a76cbc52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
